= Setting up MediaWiki =

== Why ==

Becuase of Templates and other MediaWiki features we want to go through
the original parser of MediaWiki. This is requiring a real database. This
means the articles must be imported into the database

== Installation ==

I have phase3 from trunk and use cherokee as the webserver

sudo aptitude install php5-cgi
sudo aptitude install cherokee
sudo aptitude install mime-support php5 php5-mysql mysql-server
svn co http://svn.wikimedia.org/svnroot/mediawiki/trunk/phase3
chmod a+w phase3/config
sudo vim /etc/cherokee/sites-enabled/mediawiki and put the below there
    Directory /mediawiki {
              Handler common
              DocumentRoot /dir/to/phase3-trunk
    }

sudo /etc/init.d/apache2 stop
sudo /etc/inid.d/cherokee restart

== Check installation ==
open http://127.0.0.1/mediawiki/config/index.php to test the installation. Do
you see a setup page?

== Configuring mysqlserver ==
    /etc/mysql/debian.cnf contains the admin user and pw for mysqld
    mysql -u debian-sys-maint -p
    mysql>create database wikidb;
    mysql>grant create,select, insert, update, delete, lock tables on wikidb.* to  'wikiuser'@'localhost' identified by 'PASSWORD';

== Complete the configuration ==
    Go to http:/127.0.0.1/mediawiki/config/index.php and configure
    mv the LocalSettings.php to the right place (the above website will tell you)


= Importing a dump =
== Getting a dump ==
You will need the pages and the images. Google for enwiki-latest-pages.articles.xml.bz2
and download whatever you need.
    xiangfu now use:
	 http://download.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2 
	 enwiki-latest-pages-articles.xml.bz2  2008-Jul-27 15:43:45	3.9G	application/octet-stream

== Using the scripts ==
 bzcat ../../enwiki-latest-pages-articles.xml.bz2 |  php ./maintenance/importDump.php 
    output> 100 (6.85 pages/sec 6.85 revs/sec)

  that will fail ans is very slow. splitting the text is one option, using
  http://www.mediawiki.org/wiki/MWDumper and importing the SQL dumps is
  supposed to be faster. This means that you need to configure mediawiki
  without a db prefix. But also split the content otherwise java will take
  all your memory.

= Rants and Notes =

== Import failures ==
the above script will go out of memory. wiki-tools can be used to split the dump
into smaller dump files that can be imported one by one into the database. This
also comes in handy for testing with a smaller subset.

== Other parsers ==
There is this http://www.djangosnippets.org/snippets/139/ and a Trac plugin
derived from this snippet. This looks like a rather good MediaWiki parser but
I'm not sure if it implements Templates or not.

== Performance ==
time bzcat ./enwiki-latest-pages-articles.xml.bz2 > /dev/null
real    35m11.656s
user    32m6.628s
sys     0m23.289s

time bzcat ./enwiki-latest-pages-articles.xml.bz2 | ./openmoko-wikipediareader.git/wiki-tools/wiki-tools 

real    77m41.179s
user    73m9.870s
sys     3m33.433s

== Indexfile Information ==
wc -l indexfile.index 
7278279 indexfile.index

grep -v Image indexfile.index | grep -v Category > strip.indexfile.index
du -sh strip.indexfile.index 
364M	strip.indexfile.index

wc -l strip.indexfile.index 
6057152 strip.indexfile.index

wc -w strip.indexfile.index 
22993660 strip.indexfile.index

22993660 - 6057152 = 16936508		// word count except SHA1
16936508 / 6057152 = 2.796117383	// average word count per line
sort strip.indexfile.index |grep '^[aA]' | grep ' \{1,\}' > justAfile.index 
//the test data xiangfu use now 2008-10-16 and the data just have one word per line
