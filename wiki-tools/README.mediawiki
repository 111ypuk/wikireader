= Setting up MediaWiki =

== Why ==

Becuase of Templates and other MediaWiki features we want to go through
the original parser of MediaWiki. This is requiring a real database. This
means the articles must be imported into the database

== Installation ==

I have phase3 from trunk and use cherokee as the webserver

sudo aptitude install php5-cgi
sudo aptitude install cherokee
sudo aptitude install mime-support php5 php5-mysql mysql-server
svn co http://svn.wikimedia.org/svnroot/mediawiki/trunk/phase3
chmod a+w phase3/config
sudo vim /etc/cherokee/sites-enabled/mediawiki and put the below there
    Directory /mediawiki {
              Handler common
              DocumentRoot /dir/to/phase3-trunk
    }

sudo /etc/init.d/apache2 stop
sudo /etc/inid.d/cherokee restart

== Check installation ==
open http://127.0.0.1/mediawiki/config/index.php to test the installation. Do
you see a setup page?

== Configuring mysqlserver ==
    /etc/mysql/debian.cnf contains the admin user and pw for mysqld
    mysql -u debian-sys-maint -p
    mysql>create database wikidb;
    mysql>grant create,select, insert, update, delete, lock tables on wikidb.* to  'wikiuser'@'localhost' identified by 'PASSWORD';

== Complete the configuration ==
    Go to http:/127.0.0.1/mediawiki/config/index.php and configure
    mv the LocalSettings.php to the right place (the above website will tell you)


= Importing a dump =
== Getting a dump ==
You will need the pages and the images. Google for enwiki-latest-pages.articles.xml.bz2
and download whatever you need.
    xiangfu now use:
	 http://download.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2 
	 enwiki-latest-pages-articles.xml.bz2  2008-Jul-27 15:43:45	3.9G	application/octet-stream

== Using the scripts ==
 bzcat ../../enwiki-latest-pages-articles.xml.bz2 |  php ./maintenance/importDump.php 
    output> 100 (6.85 pages/sec 6.85 revs/sec)

  that will fail ans is very slow. splitting the text is one option, using
  http://www.mediawiki.org/wiki/MWDumper and importing the SQL dumps is
  supposed to be faster. This means that you need to configure mediawiki
  without a db prefix. But also split the content otherwise java will take
  all your memory.

    java -jar mwdumper.jar --format=sql:1.5 enwiki-latest-pages-articles.xml.bz2 > wiki_sql_dump
    cat wiki_sql_dump | mysql -u wikiuser -p wikidb
    
    The wiki_sql_dump will be close to 16gb of size.

  http://meta.wikimedia.org/wiki/Data_dump has some stuff on how to install
  "tidy" to have less messed up display of the pages. Also look at the
  extensions one needs to install besides Tidy.

== After the database is imported ==
Your MediaWiki needs to enable certain extensions:

    I have a mediawiki and the extensions from trunk and symlinks work fine

    extensions/Citation -> ../../for-zecke/extensions-trunk/Citation
    extensions/Cite -> ../../for-zecke/extensions-trunk/Cite
    extensions/ImageMap -> ../../for-zecke/extensions-trunk/ImageMap
    extensions/ParserFunctions -> ../../for-zecke/extensions-trunk/ParserFunctions
    extensions/Purge -> ../../for-zecke/extensions-trunk/Purge
    extensions/README
    extensions/tidy -> ../../for-zecke/extensions-trunk/tidy

    And you need to enable some of them manually in the LocalSettings.php placed at the bottom.

    $wgUseTidy = true;
    $wgTidyBin = '/space/dports/bin/tidy';
    $wgTidyConf = $IP.'/extensions/tidy/tidy.conf';

    require_once( "$IP/extensions/ParserFunctions/ParserFunctions.php" );
    require_once( "$IP/extensions/Cite/Cite.php" );
    require_once( "$IP/extensions/ImageMap/ImageMap.php" );



= Rants and Notes =

== Import failures ==
the above script will go out of memory. wiki-tools can be used to split the dump
into smaller dump files that can be imported one by one into the database. This
also comes in handy for testing with a smaller subset.

== Other parsers ==
There is this http://www.djangosnippets.org/snippets/139/ and a Trac plugin
derived from this snippet. This looks like a rather good MediaWiki parser but
I'm not sure if it implements Templates or not.

== Performance ==
time bzcat ./enwiki-latest-pages-articles.xml.bz2 > /dev/null
real    35m11.656s
user    32m6.628s
sys     0m23.289s

time bzcat ./enwiki-latest-pages-articles.xml.bz2 | ./openmoko-wikipediareader.git/wiki-tools/wiki-tools 

real    77m41.179s
user    73m9.870s
sys     3m33.433s

== Indexfile Information ==
du -sh indexfile.index extracted.t*
55M	indexfile.index
3.5G	extracted.text
73M	extracted.titles

wc -l indexfile.index 
937757 indexfile.index

# Must read on data dumps
http://meta.wikimedia.org/wiki/Data_dumps
